{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimized Fine-tuning of Gemma-2B-IT on Colab (A100 GPU)\n",
    "\n",
    "This notebook provides a complete, optimized workflow for fine-tuning the `google/gemma-2b-it` model on a Google Colab A100 GPU. It leverages Flash Attention 2 and `bfloat16` for maximum performance and memory efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Environment\n",
    "\n",
    "We install all necessary libraries, including `flash-attn` for optimization on A100 GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q -U accelerate bitsandbytes peft transformers trl datasets huggingface_hub flash-attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log in to the Hugging Face Hub to download the model and push your fine-tuned adapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and Prepare Data\n",
    "\n",
    "We will load the same two datasets as before. The key change will be in how we format the data to match the expected input for an instruction-tuned model like `gemma-2b-it`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "# Load the datasets\n",
    "translation_dataset = load_dataset(\"michsethowusu/english-tooro_sentence-pairs_mt560\", split='train')\n",
    "multitask_dataset = load_dataset(\"cle-13/rutooro_multitask\", split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Model and Tokenizer (Optimized for A100)\n",
    "\n",
    "We now load the instruction-tuned model, `google/gemma-2b-it`. We enable Flash Attention 2 for significant speedup and memory savings on A100 GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"google/gemma-2b-it\"\n",
    "\n",
    "# Configure quantization for A100\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 # Use bfloat16 for A100\n",
    ")\n",
    "\n",
    "# Load the model with Flash Attention 2\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.padding_side = 'right'\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Formatting with Chat Templates\n",
    "\n",
    "Instead of manually creating an instruction string, we will now use the tokenizer's `apply_chat_template` method. This is the correct way to format data for instruction-tuned or chat models. It ensures the input matches the format the model was trained on. We create a function to transform our examples into the required list of dictionaries (`[{\"role\": \"user\", ...}, {\"role\": \"assistant\", ...}]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_for_chat_template(sample):\n",
    "    # Handle the multitask dataset\n",
    "    if 'instruction' in sample:\n",
    "        user_content = sample['instruction']\n",
    "        if sample.get('input'):\n",
    "            user_content += \"\\n\" + sample['input']\n",
    "        assistant_content = sample['output']\n",
    "    # Handle the translation dataset\n",
    "    elif 'en' in sample and 'tt' in sample:\n",
    "        user_content = f\"Translate this to Rutooro: {sample['en']}\"\n",
    "        assistant_content = sample['tt']\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected sample structure: {sample.keys()}\")\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "        {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "    ]\n",
    "    \n",
    "    return {\"text\": tokenizer.apply_chat_template(messages, tokenize=False)}\n",
    "\n",
    "# Apply the formatting\n",
    "formatted_translation_dataset = translation_dataset.map(format_for_chat_template)\n",
    "formatted_multitask_dataset = multitask_dataset.map(format_for_chat_template)\n",
    "\n",
    "# Merge and split\n",
    "merged_dataset = concatenate_datasets([formatted_translation_dataset, formatted_multitask_dataset])\n",
    "merged_dataset = merged_dataset.shuffle(seed=42)\n",
    "dataset_splits = merged_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = dataset_splits['train']\n",
    "test_dataset = dataset_splits['test']\n",
    "\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Testing set size: {len(test_dataset)}\")\n",
    "print(\"\\nExample entry:\\n\", train_dataset[0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Configure QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Define Training Arguments (Optimized for A100)\n",
    "\n",
    "We now use `bf16=True` to leverage the A100's Tensor Cores for faster and more stable training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"gemma-2b-it-rutooro-A100\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=25,\n",
    "    bf16=True, # A100 optimization\n",
    "    push_to_hub=True,\n",
    "    report_to=\"tensorboard\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Fine-tuning with SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    peft_config=lora_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    args=training_args,\n",
    "    max_seq_length=1024,\n",
    "    packing=True,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(f\"{training_args.output_dir}/final_adapter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Save Final Model to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "source_dir = training_args.output_dir\n",
    "destination_dir = f\"/content/drive/MyDrive/{source_dir}\"\n",
    "os.makedirs(destination_dir, exist_ok=True)\n",
    "!cp -r {source_dir}/* {destination_dir}/\n",
    "print(f\"Model saved to: {destination_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Inference\n",
    "\n",
    "To run inference, we must format our prompt using the same chat template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "prompt_text = \"Translate this to Rutooro: I am going to the market.\"\n",
    "\n",
    "# Format the prompt using the chat template\n",
    "messages = [{\"role\": \"user\", \"content\": prompt_text}]\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=50, do_sample=True, top_k=50, top_p=0.95)\n",
    "\n",
    "response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Push to Hub and Conclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()\n",
    "print(\"Notebook complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
